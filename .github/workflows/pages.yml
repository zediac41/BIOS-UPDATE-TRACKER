name: Build & Deploy Pages (manual/scheduled only)

on:
  workflow_dispatch: {}
  schedule:
    # ~5â€“6am US Central without DST fuss (11:00 UTC)
    - cron: "0 11 * * *"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Check out
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euxo pipefail
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # If your scrapers use Playwright, install browsers + system deps
          python - <<'PY'
          import importlib.util, subprocess, sys
          if importlib.util.find_spec("playwright"):
              subprocess.check_call([sys.executable, "-m", "playwright", "install", "--with-deps", "chromium"])
          PY

      - name: Build site
        run: |
          set -euxo pipefail
          python --version
          python bios_tracker.py

      - name: Verify docs exist
        run: |
          if [ ! -f docs/index.html ]; then
            echo "::error::docs/index.html not found. Ensure bios_tracker.py writes to docs/."
            exit 1
          fi
          ls -lha docs || true

      - name: Upload debug artifacts on failure
        if: ${{ failure() }}
        uses: actions/upload-artifact@v4
        with:
          name: build-debug
          path: |
            docs/**
            cache/**
            data.json
            software.html
            **/*.log
            *.log

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
